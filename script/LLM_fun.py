from typing import Tuple
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def initialize_model(checkpoint: str, 
                     device: str = "cuda") -> Tuple[AutoTokenizer, AutoModelForCausalLM]:
    """
    Initialize and return a tokenizer and model loaded from a specified Hugging Face checkpoint.

    This function loads a tokenizer and a causal language model compatible with the given checkpoint.
    The model is configured for use with a specified computing device (e.g., CPU or CUDA for GPU).
    It ensures that the model utilizes half-precision floating-point format (bfloat16) for enhanced performance
    on compatible hardware.

    Parameters:
    - checkpoint (str): The name of the pre-trained model checkpoint from Hugging Face's model hub.
    - device (str): The device to use for the model. Defaults to "cuda" for GPU acceleration.

    Returns:
    - tuple(AutoTokenizer, AutoModelForCausalLM): A tuple containing the initialized tokenizer and model.
    """
    tokenizer = AutoTokenizer.from_pretrained(checkpoint)
    model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map="auto", torch_dtype=torch.bfloat16)
    model.to(device)
    return tokenizer, model


def print_memory_footprint(model: AutoModelForCausalLM):
    """
    Print the memory footprint of the given model in megabytes (MB).

    This function retrieves the memory footprint of the model using its
    `get_memory_footprint` method, converts it to megabytes, and prints it in
    a human-readable format.

    Parameters:
    - model: An object that represents a machine learning model. This object
             must have a method `get_memory_footprint()` which returns the
             memory footprint in bytes.

    Returns:
    None. This function directly prints the memory footprint to the console.
    """
    memory_footprint_bytes = model.get_memory_footprint()
    memory_footprint_mb = memory_footprint_bytes / 1e6
    print(f"Memory footprint: {memory_footprint_mb:.2f} MB")


def generate_text(tokenizer: AutoTokenizer, 
                  model: AutoModelForCausalLM, 
                  text: str, 
                  device: str = "cuda", 
                  max_length_input: int = 500, 
                  max_new_tokens: int = 50) -> str:
    """
    Generate text based on the provided input text using a pre-initialized tokenizer and model.

    This function encodes the input text to model-compatible tokens, generates text by predicting subsequent tokens
    up to a specified maximum length, and then decodes the tokens back to a string. The generation leverages the model
    and tokenizer previously initialized with the `initialize_model` function.

    Parameters:
    - tokenizer (AutoTokenizer): The tokenizer for encoding the input text and decoding the output tokens.
    - model (AutoModelForCausalLM): The pre-initialized model used for generating the text.
    - text (str): The input text to base the generation on.
    - device (str): The device the model operates on. Defaults to "cuda" for GPU acceleration.
    - max_length_input (int): The maximum length of the input text to be encoded (in tokens). Defaults to 500.
    - max_new_tokens (int): The maximum number of new tokens to be generated. Defaults to 50.

    Returns:
    - str: The text generated by the model based on the input text.
    """
    # Encode the input text
    inputs = tokenizer.encode(text, return_tensors="pt", max_length=max_length_input).to(device)
    
    # Generate a sequence of tokens
    output_tokens = model.generate(inputs, max_new_tokens=max_new_tokens)[0]
    
    # Decode the tokens to a string, skipping special tokens
    generated_text = tokenizer.decode(output_tokens, skip_special_tokens=True)
    return generated_text

